"""===========================
Pipeline template
===========================

The purose of this pipeline is to run the QuSAR pipeline as defined in 
(Harvey et al, 2015). 

Overview
========

The following steps are applied:

1. Reads filtered for multimappers, reads mapping to regions of high or 
   low mappability and deduplicated.

2. Pile-ups are generated over the provided SNP locations (optionally
   intersected with regions of interest, such as transcript or peak
   locations)..

3. Pileups are reformatted as per quasar documentation

4. The QuASAR analysis is executed in R, which co-estimates the which
   SNPs are hetrozygous and which of these have evidence for ASE. 


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use cgat pipelines.

Replicates
----------

The are two different levels of replication: 

    * Multiple samples taken from the same indevidual or cell line
    * Samples from multiple indeviduals. 

Currently the pipeline will run multiple sampels from the same indevidual
together, and those from seperate indeviduals in different runs. 

It does need to know how to tell. By default, the regular expression
`(.+?-.+?)-.+\..+.bam` is used, and samples with the same group 1
are assumed to be replicates of the same indevidual, although this can be 
configured in the `pipeline.yml`.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_quasar.py config

The key things that need to be configured are the name the bedfile with 
SNP locations, regions of high and low mappability, of the regions of 
interest file (if any), and regex to identify replicates.

Input files
-----------

* BAM files, named in the 1-2-3.bam naming scheme
* A file with the locations and MAFs of SNPs of interest
* (Optionally) a bed or GTF file with regions of interest

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_genesets`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default cgat setup, the pipeline requires the following
software to be in the path:

Requirements:

* samtools >= 1.1
* Picard
* Bedtools
* R, Bioconductor and the QuASAR R pacakge

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
import sys
import re
import os
from ruffus import transform, regex, suffix, follows, formatter, add_inputs, merge
from cgatcore import pipeline as P
from cgatcore import iotools
# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


# -----------------------------------------------------------------------------
@transform("*.bam",
           formatter(),
           add_inputs(PARAMS["regions_of_high_mappability"],
                      PARAMS["regions_of_low_mappability"],
                      PARAMS["regions_of_interest"]),
           r"filtered_bams.dir/{basename[0]}.bam")
def filter_bamfiles(infiles, outfile):

    inreads, high_mapability, low_mapability, roi = infiles

    # filter not in regions of low or high mappability
    statement = ''' bedtools intersect -abam %(inreads)s -b %(high_mapability)s -v
                  | bedtools intersect -abam - -b %(low_mapability)s -v '''

    # filter only reads in regions of interest if specified
    if roi:
        statement += '''
                  | bedtools intersect -u -abam - -b %(roi)s'''

    # filter only primary, proper paired reads
    statement += '''
                  | samtools sort -n 
                  | samtools view -hF 3852'''

    # filter for uniquely mapping reads, keeping the header
    statement += '''
                  | grep -P '^@|NH:i:1'
                  | samtools view -b '''

    # remove reads that are now improperly, save sorted
    # results to a temporary file
 
    statement += '''
                  | samtools fixmate - -
                  | samtools sort -o %(outfile)s'''

    job_memory = "12G"

    P.run(statement)

    job_memory = "4G"
    
    P.run("samtools index %(outfile)s")

    
@transform(filter_bamfiles,
           formatter(),
           r"deduplicated_bams.dir/{basename[0]}.bam")
def deduplicate_reads(infile, outfile):
    
    tmpdir = P.get_temp_dir(dir=PARAMS["shared_tmpdir"])
    
    statement = '''
                    MarkDuplicates
                    I=%(infile)s
                    O=%(outfile)s
                    ASSUME_SORTED=True
                    VALIDATION_STRINGENCY=LENIENT
                    METRICS_FILE=%(outfile)s.stats
                    REMOVE_DUPLICATES=True
                    TMP_DIR=%(tmpdir)s > %(outfile)s.log

                    && 

                    samtools index %(outfile)s

                    && 

                    rm %(tmpdir)s -r '''

    
    job_memory = "16G"

    P.run(statement)

    
# -----------------------------------------------------------------------------
@transform(PARAMS["snps_vcf"],
           formatter(),
           add_inputs(PARAMS["regions_of_interest"]),
           "filtered_snps.bed")
def filter_snps(infiles, outfile):

    snps_bed, roi = infiles

    statement = '''zcat %(snps_bed)s'''
    if roi:

        if roi.endswith(".gz"):
            cat = "zcat " + roi
            roi = P.snip(roi, ".gz")
        else:
            cat = "cat " + roi

        if roi.endswith(".bed"):
            sort_cmd = "sort -k1,1V -k2,2n"
        elif roi.endswith(".gtf"):
            sort_cmd = "sort -k1,1V -k4,4n"
            
        statement += '''
                 | bedtools intersect -u -a stdin 
                                      -b <(%(cat)s | sed 's/^chr//' | %(sort_cmd)s) 
                                      -g <(%(cat)s | sed 's/^chr//' | %(sort_cmd)s | cut -f1 | uniq | awk -v OFS='\\t' '{print $1, "1"'})
                                      -sorted'''
  
    statement += '''
                 | sed -E 's/^([1-9MXY])/chr\\1/'
                 | grep -P '^chr'
                 | awk -v OFS='\\t' '$5 ~ /^[ATGC]$/ && $4 ~  /^[ATGC]$/'
                 | sed -E 's/\\t[^\\t]+;CAF=[0-9\\.]+,([0-9\\.]+);.+/\\t\\1/'
                 | awk -v OFS='\\t' '{print $1, $2-1, $2, $3, $4, $5, $8}'
                  > %(outfile)s'''

    P.run(statement)

    
# -----------------------------------------------------------------------------
@transform(deduplicate_reads,
           formatter(),
           add_inputs(filter_snps),
           "pileups.dir/{basename[0]}.pileup.bed.gz")
def get_pileups(infiles, outfile):

    bam, snps = infiles

    statement = ''' samtools mpileup -f %(genome_fasta)s -l %(snps)s %(bam)s
                 |  awk -v OFS='\\t' '{ if ($4>0 && 
                                            $5 !~ /[^\\^][<>]/ && 
                                            $5 !~ /\\+[0-9]+[ACGTNacgtn]+/ && 
                                            $5 !~ /-[0-9]+[ACGTNacgtn]+/ && 
                                            $5 !~ /[^\\^]\\*/) 
                                           print $1,$2-1,$2,$3,$4,$5,$6}' 
                 | sort -k1,1 -k2,2n
                 | bedtools intersect -a - -b <(sort -k1,1 -k2,2n %(snps)s) -wo
                                      -sorted
                 | cut -f 1-7,11-14
                 | gzip > %(outfile)s'''

    P.run(statement)

    
# -----------------------------------------------------------------------------
@P.cluster_runnable
def pileup_to_quasar(infile, outfile):
    import collections
    prev_line = None
    line_buffer = list()

    outf = iotools.open_file(outfile, "w")
   
    fates = collections.Counter()
    
    for line in iotools.open_file(infile):
        fields = line.strip().split("\t")

        if not fields[3].upper() == fields[8].upper():
            fates["error in bases"] += 1
            continue

        if not (int(PARAMS["min_depth"]) <= int(fields[4]) <= int(PARAMS["max_depth"])):
            fates["bad read coverage"] += 1
            continue
        
        filt_alleles = re.sub('[a-zA-z\.\, ]\$', '', fields[5])
        filt_alleles = re.sub('\^..', '', filt_alleles)
        if len(filt_alleles) == 0:
            fates["read ends only"] += 1
            continue

        alleles = re.sub('[\.\, ]', fields[3], fields[5])
        alleles = alleles.upper()
        ref = fields[3].upper()
        alt = fields[9].upper()

        ref_count = alleles.count(ref)
        alt_count = alleles.count(alt)

        outline = "\t".join([fields[0],
                             fields[1],
                             fields[2],
                             ref,
                             alt,
                             fields[7],
                             fields[10],
                             str(ref_count),
                             str(alt_count),
                             str(int(fields[4]) - ref_count - alt_count)])

        if (fields[0], fields[1]) == prev_line:
            line_buffer.append(outline)
        else:
            if len(line_buffer) == 1:
                outf.write(line_buffer[0] + "\n")
                fates["output"] += 1
            else:
                fates["duplicate lines"] += 1
            line_buffer = [outline]
            prev_line = (fields[0], fields[1])
            
    outf.close()
    log = iotools.open_file(outfile+".log", "w")
    for key in fates.keys():
        log.write("\t".join((key, str(fates[key])))+"\n")
    log.close()
    


@transform(get_pileups,
           formatter(),
           "pileups.dir/{basename[0]}.quasarin.gz")
def run_pileup_to_quasar(infile, outfile):

    pileup_to_quasar(infile, outfile, submit=True)


@transform(run_pileup_to_quasar,
           formatter(),
           r"quasar_out.dir/{basename[0]}.tsv")
def run_quasar(infile, outfile):

    pipeline_dir = os.path.dirname(__file__)
    script = os.path.join(pipeline_dir, "run_quasar.R")

    statement = '''Rscript %(script)s -I %(infile)s
                                      -O %(outfile)s
                       
                   > %(outfile)s.log'''
    P.run(statement)


# -----------------------------------------------------------------------------
@merge(run_quasar, "quasar_results.load")
def load_quasar(infiles, outfile):

    P.concatenate_and_load(infiles, outfile,
                           regex_filename="quasar_out.dir/(.+).pileup.bed.quasarin.tsv",
                           options="-i track -i annotations.rsID -i annotations.chr -i annotations.pos0 --replace-header",
                           header="track,rsID,chr,pos,betas,beta_ses,pval")
    

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
